{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fac031a-df9e-4e09-a534-696712aecf8f",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://flask-sqlalchemy.readthedocs.io/en/stable/\n",
    "* https://docs.sqlalchemy.org/en/20/orm/inheritance.html#concrete-table-inheritance\n",
    "* https://docs.sqlalchemy.org/en/20/_modules/examples/performance/bulk_inserts.html\n",
    "* https://docs.sqlalchemy.org/en/20/orm/large_collections.html#bulk-insert-of-new-items\n",
    "* https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549210ef-e66e-4461-9dd3-7a1a1f1d04f7",
   "metadata": {},
   "source": [
    "## Refactoring notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b9e2d-6f5a-4e09-a561-b029b2194010",
   "metadata": {},
   "source": [
    "- Dans le modèle SQLAlchmey PremiumFile, il manque la relation à une analyse (mais la table d'association existe bien). A ajouter :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b735abf-95e8-4058-9e72-4c8018dcd9dd",
   "metadata": {},
   "source": [
    "```python\n",
    "class PremiumFile(CommonMixin, Base):\n",
    "    \"\"\"Represents a historical premium file.\"\"\"\n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"))\n",
    "    client: Mapped[\"Client\"] = relationship(back_populates=\"premiumfiles\")\n",
    "\n",
    "    # TODO: To be added\n",
    "    analyses: Mapped[List[Analysis]] = relationship(\n",
    "        secondary=lambda: analysis_premiumfile_table, back_populates=\"premiumfiles\"\n",
    "    )\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0b18cfc-7e7e-431a-847f-2765019ef6c4",
   "metadata": {},
   "source": [
    "- Dans le modèle SQLAlchemy FrequencySeverityModel, il manque la relation avec le premium file en input du modèle. A ajouter :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b421cd3-d121-46a0-ad91-4bd064bd2642",
   "metadata": {},
   "source": [
    "```python\n",
    "class FrequencySeverityModel(ModelFile):\n",
    "    id: Mapped[int] = mapped_column(ForeignKey(\"modelfile.id\"), primary_key=True)\n",
    "    threshold: Mapped[int] = mapped_column(nullable=False)\n",
    "    lossfile_id: Mapped[int] = mapped_column(ForeignKey(\"histolossfile.id\"))\n",
    "    lossfile: Mapped[\"HistoLossFile\"] = relationship()\n",
    "    premiumfile_id: Mapped[int] = mapped_column(ForeignKey(\"premiumfile.id\"))  # TODO: To be added\n",
    "    premiumfile: Mapped[\"PremiumFile\"] = relationship()  # TODO: To be added\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05342e-b1d8-4ba5-b162-e21955deef92",
   "metadata": {},
   "source": [
    "- Dans la route qui crée un frequency-severity model file, il manque l'association du premium file au model file :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040490e-b235-4187-9cdb-cbb45a34e4ab",
   "metadata": {},
   "source": [
    "```python\n",
    "def create_frequency_severity_model:\n",
    "# Create the frequency-severity model\n",
    "        modelfile = FrequencySeverityModel(\n",
    "            model_type=\"frequency_severity_model\",\n",
    "            threshold=threshold,\n",
    "            years_simulated=years_simulated,\n",
    "            lossfile_id=lossfile_id,\n",
    "            premiumfile_id=premiumfile_id,  # TODO: To be added\n",
    "            frequencymodel=frequencymodel,\n",
    "            severitymodel=severitymodel,\n",
    "        )\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45ca9ca-3d92-45ed-97d4-c1e7130273a2",
   "metadata": {},
   "source": [
    "- Dans les relationships premiumfiles, histolossfiles et modelfiles du modèle SQLAlchemy Analysis, il manque les back-populates vers analysis, ce qui crée une anomalie lors de la suppression d'un premiumfile, histolossfile ou modelfile :\n",
    "```\n",
    "Database error occurred while deleting HistoLossFile record: (psycopg2.errors.ForeignKeyViolation) ERREUR:  UPDATE ou DELETE sur la table « histolossfile » viole la contrainte de clé étrangère « analysis_histolossfile_histolossfile_id_fkey » de la table « analysis_histolossfile »\n",
    "DETAIL:  La clé (id)=(1) est toujours référencée à partir de la table « analysis_histolossfile ».\n",
    "```\n",
    "- => A Ajouter :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12788cb3-ca8f-4534-97b2-4ea3fca4e51b",
   "metadata": {},
   "source": [
    "```python\n",
    "class Analysis(CommonMixin, Base):\n",
    "    premiumfiles: Mapped[List[\"PremiumFile\"]] = relationship(\n",
    "        secondary=lambda: analysis_premiumfile_table, back_populates=\"analyses\",    # TODO: Add back_populates\n",
    "    )\n",
    "    \n",
    "    histolossfiles: Mapped[List[\"HistoLossFile\"]] = relationship(\n",
    "        secondary=lambda: analysis_histolossfile_table, back_populates=\"analyses\"  # TODO: Add back_populates\n",
    "    )\n",
    "    modelfiles: Mapped[List[\"ModelFile\"]] = relationship(\n",
    "        secondary=lambda: analysis_modelfile_table, back_populates=\"analyses\"  # TODO: Add back_populates\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004b80d-bd33-47b4-972f-67a3612878d2",
   "metadata": {},
   "source": [
    "- De même, dans les modèles SQLAlchemy PremiumFile, HistoLossFile et ModelFile, il manque les relationships vers analysis. A ajouter :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fb96a-4a0d-4aaf-863a-5479e70f6fc7",
   "metadata": {},
   "source": [
    "```python\n",
    "class PremiumFile(CommonMixin, Base):\n",
    "    analyses: Mapped[List[Analysis]] = relationship(\n",
    "        secondary=lambda: analysis_premiumfile_table, back_populates=\"premiumfiles\"\n",
    "    )  # TODO: To be added\n",
    "    \n",
    "class HistoLossFile(CommonMixin, Base):\n",
    "    analyses: Mapped[List[Analysis]] = relationship(\n",
    "        secondary=lambda: analysis_histolossfile_table, back_populates=\"histolossfiles\"\n",
    "    )  # TODO: To be added\n",
    "\n",
    "class ModelFile(CommonMixin, Base):\n",
    "    analyses: Mapped[List[Analysis]] = relationship(\n",
    "        secondary=lambda: analysis_modelfile_table, back_populates=\"modelfiles\"\n",
    "    )  # TODO: To be added\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162c4ef-7963-421d-98ee-8eb5d82d2283",
   "metadata": {},
   "source": [
    "- The Pydantic classes FrequencyInput and SeverityInput need to be reviewed and refactored\n",
    "- Start frequency and severity models parameters with index 0 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef1fec-3dbb-437f-b996-dd7f8cab23f9",
   "metadata": {},
   "source": [
    "```python\n",
    "class FrequencyModel(CommonMixin, Base):\n",
    "    parameter_0: Mapped[float] = mapped_column(nullable=False)  # TODO: To be added\n",
    "    parameter_1: Mapped[float]\n",
    "    parameter_2: Mapped[float]\n",
    "    parameter_3: Mapped[float]\n",
    "    parameter_4: Mapped[float]\n",
    "    # parameter_5: Mapped[float]  # TODO: To be deleted\n",
    "\n",
    "class SeverityModel(CommonMixin, Base):\n",
    "    parameter_0: Mapped[float] = mapped_column(nullable=False)  # TODO: To be added\n",
    "    parameter_1: Mapped[float]\n",
    "    parameter_2: Mapped[float]\n",
    "    parameter_3: Mapped[float]\n",
    "    parameter_4: Mapped[float]\n",
    "    # parameter_5: Mapped[float]  # TODO: To be deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9fc60-6200-445f-a5eb-2168309a3451",
   "metadata": {},
   "source": [
    "- Utiliser les valeurs ModelType pour définir les identités polymorphiques liées au modèle SQLAlchemy ModelFile :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e2861-4393-4c2d-8858-fe812ae9ef6c",
   "metadata": {},
   "source": [
    "```python\n",
    "class ModelType(Enum):  # TODO: To be added\n",
    "    \"\"\"Defines the supported loss models.\"\"\"\n",
    "\n",
    "    EMPIRICAL = \"empirical\"\n",
    "    FREQUENCY_SEVERITY = \"frequency_severity\"  # TODO: Improve the value with underscores\n",
    "    COMPOSITE_FREQUENCY_SEVERITY = \"composite_frequency_severity\"\n",
    "    EXPOSURE_BASED = \"exposure_based\"\n",
    "\n",
    "\n",
    "class ModelFile(CommonMixin, Base):\n",
    "\n",
    "\n",
    "class EmpiricalModel(ModelFile):\n",
    "    __mapper_args__ = {\n",
    "        \"polymorphic_identity\": ModelType.EMPIRICAL,  # TODO: To be corrected\n",
    "    }\n",
    "\n",
    "\n",
    "class FrequencySeverityModel(ModelFile):\n",
    "    __mapper_args__ = {\n",
    "        \"polymorphic_identity\": ModelType.FREQUENCY_SEVERITY,  # TODO: To be corrected\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0318c-3ba2-4d6b-adfa-79e6239cc3ef",
   "metadata": {},
   "source": [
    "```python\n",
    "from db.models import (\n",
    "    Analysis,\n",
    "    Client,\n",
    "    FrequencyModel,\n",
    "    FrequencySeverityModel,\n",
    "    HistoLossFile,\n",
    "    ModelType,  # TODO: To be added\n",
    "    ModelYearLoss,\n",
    "    PremiumFile,\n",
    "    SeverityModel,\n",
    ")\n",
    "\n",
    "def create_frequency_severity_model\n",
    "        # Create the frequency-severity model\n",
    "        modelfile = FrequencySeverityModel(\n",
    "            model_type=ModelType.FREQUENCY_SEVERITY.value,  # TODO: To be corrected\n",
    "            threshold=threshold,\n",
    "            years_simulated=years_simulated,\n",
    "            lossfile_id=lossfile_id,\n",
    "            premiumfile_id=premiumfile_id,\n",
    "            frequencymodel=frequencymodel,\n",
    "            severitymodel=severitymodel,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54af16-f6a3-49d4-b4c2-e86a16d8329c",
   "metadata": {},
   "source": [
    "- **Confusion entre input et output (cf. https://gitlab.com/ccr-re-df/products/app/backends/tarification-nonvie-backend/-/blob/dev/app/api/routes/model.py) :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6269ea9-27b7-4683-98f8-536aaf5d372d",
   "metadata": {},
   "source": [
    "```python\n",
    "async def generate_stochastic_year_loss_table_and_metadata(\n",
    "    frequence_model_input: FrequencyModelOutput,\n",
    "    severity_model_input: SeverityModelOutput,\n",
    "    frequence_severity_model_input: FrequencySeverityModelInputExtend,\n",
    "    threshold_input: float,\n",
    "    analysis_id: int,\n",
    "    user_id: int,\n",
    "    lossfile_id: int,\n",
    "    injected_model_service: ModelServiceDep,\n",
    ") -> StochasticModelRouteResponse:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d08190-79d0-42a3-8372-a008665f9d85",
   "metadata": {},
   "source": [
    "- Finally, create **specific** issues in Jira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd4bb3-3492-4dd5-95f9-bf2380809997",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3419ccd-9220-4e19-a851-ca33b5190946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from engine.model.frequency_severity import (\n",
    "    DistributionInput,\n",
    "    DistributionType,\n",
    "    get_modelyearloss_frequency_severity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ecd867-201c-4729-8d42-2bfa52208eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration = 0.1773916999809444\n",
      "Average Loss = 1986.6839583027847\n",
      "Frequency = 3.00068\n",
      "shape: (300_068, 11)\n",
      "┌───────┬─────┬──────┬───────────┬───┬────────────┬────────┬──────────────────┬──────────────┐\n",
      "│ year  ┆ day ┆ loss ┆ loss_type ┆ … ┆ model_hash ┆ model  ┆ line_of_business ┆ modelfile_id │\n",
      "│ ---   ┆ --- ┆ ---  ┆ ---       ┆   ┆ ---        ┆ ---    ┆ ---              ┆ ---          │\n",
      "│ i64   ┆ i32 ┆ i64  ┆ str       ┆   ┆ object     ┆ object ┆ object           ┆ i64          │\n",
      "╞═══════╪═════╪══════╪═══════════╪═══╪════════════╪════════╪══════════════════╪══════════════╡\n",
      "│ 0     ┆ 152 ┆ 1534 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 0     ┆ 261 ┆ 1232 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 0     ┆ 91  ┆ 1699 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 0     ┆ 91  ┆ 1923 ┆ non_cat   ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 1     ┆ 61  ┆ 1000 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ …     ┆ …   ┆ …    ┆ …         ┆ … ┆ …          ┆ …      ┆ …                ┆ …            │\n",
      "│ 99999 ┆ 142 ┆ 1257 ┆ non_cat   ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 99999 ┆ 232 ┆ 1126 ┆ non_cat   ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 99999 ┆ 69  ┆ 1653 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 99999 ┆ 249 ┆ 5341 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "│ 99999 ┆ 172 ┆ 1352 ┆ cat       ┆ … ┆ null       ┆ null   ┆ null             ┆ 1            │\n",
      "└───────┴─────┴──────┴───────────┴───┴────────────┴────────┴──────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "threshold = 1000\n",
    "\n",
    "frequency_input = DistributionInput(\n",
    "    dist=DistributionType.POISSON,\n",
    "    threshold=threshold,\n",
    "    params=[3],\n",
    ")\n",
    "\n",
    "severity_input = DistributionInput(\n",
    "    dist=DistributionType.PARETO,\n",
    "    threshold=threshold,\n",
    "    params=[2],\n",
    ")\n",
    "\n",
    "cat_share = 0.5\n",
    "simulated_years = 100_000\n",
    "modelfile_id = 1\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "modelyearloss = get_modelyearloss_frequency_severity(\n",
    "    frequency_input,\n",
    "    severity_input,\n",
    "    cat_share,\n",
    "    simulated_years,\n",
    "    modelfile_id,\n",
    ")\n",
    "\n",
    "print(f\"Duration = {time.perf_counter() - start}\")\n",
    "print(f\"Average Loss = {modelyearloss[\"loss\"].mean()}\")\n",
    "print(f\"Frequency = {len(modelyearloss[\"loss\"]) / simulated_years}\")\n",
    "print(modelyearloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846cfc-7582-4e2d-9e4d-0c700173f432",
   "metadata": {},
   "source": [
    "## Test Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3ca2d7-9b01-49a0-9eb9-17a2ac38dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from db.crud import (\n",
    "    create_analysis,\n",
    "    create_client,\n",
    "    create_frequency_severity_model,\n",
    "    create_historical_loss_file,\n",
    "    create_premium_file,\n",
    "    delete_db_record,\n",
    ")\n",
    "from db.models import (\n",
    "    Analysis,\n",
    "    Client,\n",
    "    FrequencyModel,\n",
    "    FrequencySeverityModel,\n",
    "    HistoLossFile,\n",
    "    ModelFile,\n",
    "    ModelYearLoss,\n",
    "    PremiumFile,\n",
    "    SeverityModel,\n",
    "    session,\n",
    ")\n",
    "from engine.model.frequency_severity import DistributionInput, DistributionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d3c58f-6cb1-4f42-9c2c-1e4a4feab07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 'AXA' with ID 1 added successfully.\n",
      "Analysis with ID 1 added successfully.\n",
      "Premium file with ID 1 added successfully.\n",
      "Historical loss file with ID 1 added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a client, an analysis, a premium file and a historical loss file\n",
    "create_client(session, client_name=\"AXA\")\n",
    "create_analysis(session, client_id=1)\n",
    "create_premium_file(session, analysis_id=1)\n",
    "create_historical_loss_file(session, analysis_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396fc813-c224-4469-b374-3e9c7d6d3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create model file: 0.02 seconds\n",
      "Time to flush the session: 0.00 seconds\n",
      "Time to generate year loss data: 0.08 seconds\n",
      "Time to insert year loss records into database: 1.28 seconds\n",
      "Time to commit transaction: 0.01 seconds\n",
      "Frequency-Severity Model with ID 1 created successfully.\n",
      "Total Duration = 1.385781099786982\n"
     ]
    }
   ],
   "source": [
    "# Create a frequency-severity model\n",
    "start = time.perf_counter()\n",
    "create_frequency_severity_model(\n",
    "    session,\n",
    "    analysis_id=1,\n",
    "    lossfile_id=1,\n",
    "    premiumfile_id=1,\n",
    "    threshold=1000,\n",
    "    frequency_input=DistributionInput(\n",
    "        dist=DistributionType.POISSON,\n",
    "        threshold=1000,\n",
    "        params=[3, 0, 0, 0, 0],\n",
    "    ),\n",
    "    severity_input=DistributionInput(\n",
    "        dist=DistributionType.PARETO,\n",
    "        threshold=1000,\n",
    "        params=[2, 0, 0, 0, 0],\n",
    "    ),\n",
    "    cat_share=0.5,\n",
    "    years_simulated=10_000,\n",
    ")\n",
    "\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Total Duration = {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648a05f5-94f1-4425-85d3-c0b97e9e2202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistoLossFile record with ID 1 has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# DbModel = Analysis\n",
    "# DbModel = PremiumFile\n",
    "DbModel = HistoLossFile\n",
    "# DbModel = FrequencyModel\n",
    "# DbModel = SeverityModel\n",
    "# DbModel = FrequencySeverityModel\n",
    "\n",
    "delete_db_record(session, DbModel, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3664ba-ffea-49b9-9bfe-68c1bd0d818e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis record with ID 2 has been deleted.\n"
     ]
    }
   ],
   "source": [
    "DbModel = Analysis\n",
    "# DbModel = PremiumFile\n",
    "# DbModel = HistoLossFile\n",
    "# DbModel = FrequencyModel\n",
    "# DbModel = SeverityModel\n",
    "# DbModel = FrequencySeverityModel\n",
    "\n",
    "delete_db_record(session, DbModel, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7e829-4795-48d8-9856-e3d3facf2c13",
   "metadata": {},
   "source": [
    "- Problem with the deletion of frequency and severity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153442c4-d952-48af-a3bd-c598cc0cc6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
