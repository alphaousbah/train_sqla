{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fac031a-df9e-4e09-a534-696712aecf8f",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://flask-sqlalchemy.readthedocs.io/en/stable/\n",
    "* https://docs.sqlalchemy.org/en/20/orm/inheritance.html#concrete-table-inheritance\n",
    "* https://docs.sqlalchemy.org/en/20/_modules/examples/performance/bulk_inserts.html\n",
    "* https://docs.sqlalchemy.org/en/20/orm/large_collections.html#bulk-insert-of-new-items\n",
    "* https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549210ef-e66e-4461-9dd3-7a1a1f1d04f7",
   "metadata": {},
   "source": [
    "## Refactoring notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a5368-e679-4afb-bc7a-d6bc7966109b",
   "metadata": {},
   "source": [
    "- The relationship between a frequency severity model and the input premium file is missing\n",
    "- The back-relationship were missing for handling properly session.delete(histolossfile) :\n",
    "\n",
    "-\n",
    "\n",
    "    class Analysis(CommonMixin, Base):\n",
    "        \"\"\"Represents an analysis entity.\"\"\"\n",
    "    \n",
    "        id: Mapped[int] = mapped_column(primary_key=True)\n",
    "        client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "        client: Mapped[\"Client\"] = relationship(back_populates=\"analyses\")\n",
    "    \n",
    "        histolossfiles: Mapped[List[\"HistoLossFile\"]] = relationship(\n",
    "            secondary=lambda: analysis_histolossfile_table, back_populates=\"analyses\"\n",
    "        )\n",
    "        modelfiles: Mapped[List[\"ModelFile\"]] = relationship(\n",
    "            secondary=lambda: analysis_modelfile_table, back_populates=\"analyses\"\n",
    "        )\n",
    "\n",
    "        class HistoLossFile(CommonMixin, Base):\n",
    " \n",
    "        analyses: Mapped[List[Analysis]] = relationship(\n",
    "            secondary=lambda: analysis_histolossfile_table, back_populates=\"histolossfiles\"\n",
    "        )\n",
    "\n",
    "  - \n",
    "\n",
    "      class ModelFile(CommonMixin, Base):\n",
    "            \"\"\"Base class for model files.\"\"\"\n",
    "        \n",
    "            id: Mapped[int] = mapped_column(primary_key=True)\n",
    "            model_type: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "            years_simulated: Mapped[int] = mapped_column(nullable=False)\n",
    "        \n",
    "            client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "            client: Mapped[\"Client\"] = relationship(back_populates=\"modelfiles\")\n",
    "        \n",
    "            yearlosses: Mapped[List[\"ModelYearLoss\"]] = relationship(\n",
    "                back_populates=\"modelfile\",\n",
    "                cascade=\"all, delete-orphan\",\n",
    "            )\n",
    "        \n",
    "            analyses: Mapped[List[Analysis]] = relationship(\n",
    "                secondary=lambda: analysis_modelfile_table, back_populates=\"modelfiles\"\n",
    "            )\n",
    "    \n",
    "\n",
    "- Cascade delete, all for client-analysis:\n",
    "\n",
    "\n",
    "    class Client(CommonMixin, Base):\n",
    "        \"\"\"Represents a client entity.\"\"\"\n",
    "    \n",
    "        id: Mapped[int] = mapped_column(primary_key=True)\n",
    "        name: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "    \n",
    "        analyses: Mapped[List[\"Analysis\"]] = relationship(\n",
    "            back_populates=\"client\", cascade=\"all, delete-orphan\"\n",
    "        )\n",
    "\n",
    "- The Pydantic classes FrequencyInput and SeverityInput need to be reviewed and refactored\n",
    "- The attribute treshold is missing in the FrequencyModel class\n",
    "- Start frequency and severity models parameters with index 0\n",
    "- The threshold of frequency and severity models is that of the related frequency_severity_model => Remove attribute threshold from severity model\n",
    "- Use ModelType in polymorphic identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd4bb3-3492-4dd5-95f9-bf2380809997",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd7511-27f7-4d3a-bbd8-464a1aa82cea",
   "metadata": {},
   "source": [
    "### Enumerations and Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df406879-7e64-4964-b0c1-52e604df0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ModelType(Enum):\n",
    "    \"\"\"Defines the supported loss models.\"\"\"\n",
    "\n",
    "    EMPIRICAL = \"empirical\"\n",
    "    # FREQUENCY_SEVERITY = \"frequency_severity\"\n",
    "    FREQUENCY_SEVERITY = \"frequencyseveritymodel\"\n",
    "    COMPOSITE_FREQUENCY_SEVERITY = \"composite_frequency_severity\"\n",
    "    EXPOSURE_BASED = \"exposure_based\"\n",
    "\n",
    "\n",
    "class DistributionType(Enum):\n",
    "    \"\"\"Defines the supported statistical distributions.\"\"\"\n",
    "\n",
    "    POISSON = \"poisson\"\n",
    "    NEGATIVE_BINOMIAL = \"negative_binomial\"\n",
    "    PARETO = \"pareto\"\n",
    "\n",
    "\n",
    "class LossType(Enum):\n",
    "    \"\"\"Defines the loss types.\"\"\"\n",
    "\n",
    "    CAT = \"cat\"\n",
    "    NON_CAT = \"non_cat\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DistributionInput:\n",
    "    \"\"\"\n",
    "    Configuration for a statistical distribution.\n",
    "\n",
    "    Attributes:\n",
    "        dist: The distribution type (enum).\n",
    "        params: Parameters specific to the distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    dist: DistributionType\n",
    "    params: list[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b719d63-a33c-486c-9310-942158abbf19",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5ecd867-201c-4729-8d42-2bfa52208eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen distribution: pareto\n",
      "Parameters: [2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.stats import poisson, nbinom, pareto\n",
    "\n",
    "\n",
    "# Main function to generate model year loss\n",
    "def get_modelyearloss_frequency_severity(\n",
    "    threshold: float,\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    simulated_years: int,\n",
    "    modelfile_id: int,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate loss data for a frequency-severity model over a number of simulated years.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Multiplier applied to the severity losses.\n",
    "        frequency_input (DistributionInput): Distribution defining the frequency of events per year.\n",
    "        severity_input (DistributionInput): Distribution defining the severity of each event.\n",
    "        simulated_years (int): Number of years to simulate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"year\": List of years for each loss event.\n",
    "            - \"day\": Random day of the year for each loss.\n",
    "            - \"loss\": Calculated loss values.\n",
    "            - \"loss_type\": Type of loss (e.g., catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    frequencies = generate_frequencies(frequency_input, simulated_years)\n",
    "    years = generate_years(frequencies)\n",
    "    loss_count = len(years)\n",
    "    days = generate_days(loss_count)\n",
    "    losses = generate_losses_from_parametric_dist(threshold, severity_input, loss_count)\n",
    "    loss_types = generate_loss_types(loss_count)\n",
    "    modelfile_ids = np.repeat(modelfile_id, loss_count)\n",
    "\n",
    "    modelyearloss = pl.DataFrame(\n",
    "        {\n",
    "            \"year\": years,\n",
    "            \"day\": days,\n",
    "            \"loss\": losses,\n",
    "            \"loss_type\": loss_types,\n",
    "            \"modelfile_id\": modelfile_ids,\n",
    "        }\n",
    "    )\n",
    "    return modelyearloss\n",
    "\n",
    "\n",
    "def generate_frequencies(\n",
    "    frequency_input: DistributionInput,\n",
    "    size: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a list of event frequencies based on a specified distribution.\n",
    "\n",
    "    Args:\n",
    "        frequency_input (DistributionInput): The distribution and parameters for frequency generation.\n",
    "        size (int): Number of values to generate.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of event frequencies for each simulated year.\n",
    "    \"\"\"\n",
    "    frequencies = get_sample_from_dist(frequency_input, size)\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def generate_years(frequencies: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate a list of years for loss events based on event frequencies.\n",
    "\n",
    "    Args:\n",
    "        frequencies (list[int]): A list where each element represents the number of events in a year.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of years, repeated according to their respective frequencies.\n",
    "    \"\"\"\n",
    "    years = [\n",
    "        year for year, freq in enumerate(frequencies, start=1) for _ in range(freq)\n",
    "    ]\n",
    "    return years\n",
    "\n",
    "\n",
    "def generate_days(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random days of the year for loss events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of days to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of random integers representing days (1 to 365).\n",
    "    \"\"\"\n",
    "    days = np.random.randint(1, 366, size)\n",
    "    return days\n",
    "\n",
    "\n",
    "def generate_loss_types(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random loss types (catastrophic or non-catastrophic) for events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of loss types to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of randomly chosen loss types (catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    loss_types = np.random.choice([LossType.CAT.value, LossType.NON_CAT.value], size)\n",
    "    return loss_types\n",
    "\n",
    "\n",
    "def generate_losses_from_parametric_dist(\n",
    "    threshold: int,\n",
    "    severity_input: DistributionInput,\n",
    "    loss_count: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a set of rounded loss values from a parametric distribution.\n",
    "\n",
    "    Args:\n",
    "        threshold (int): A threshold value (not used in the current implementation).\n",
    "        severity_input (DistributionInput): Parameters defining the severity distribution.\n",
    "        loss_count (int): The number of loss values to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of rounded loss values.\n",
    "    \"\"\"\n",
    "    sample = threshold * get_sample_from_dist(severity_input, loss_count)\n",
    "    return np.round(sample)\n",
    "\n",
    "\n",
    "def get_sample_from_dist(\n",
    "    distribution_input: DistributionInput, size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a sample from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution_input (DistributionInput): An object containing the distribution type\n",
    "            (e.g., Poisson, Negative Binomial, Pareto) and its associated parameters.\n",
    "        size (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of samples drawn from the specified distribution.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the distribution type is not supported.\n",
    "    \"\"\"\n",
    "    dist = distribution_input.dist\n",
    "    params = distribution_input.params\n",
    "\n",
    "    match dist:\n",
    "        case DistributionType.POISSON:\n",
    "            return poisson.rvs(mu=params[0], size=size)\n",
    "        case DistributionType.NEGATIVE_BINOMIAL:\n",
    "            return nbinom.rvs(n=params[0], p=params[1], size=size)\n",
    "        case DistributionType.PARETO:\n",
    "            return pareto.rvs(b=params[0], size=size)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported distribution: {dist}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "distribution_input = DistributionInput(dist=DistributionType.PARETO, params=[2])\n",
    "sample = 1000 * get_sample_from_dist(distribution_input, size=1_000_000)\n",
    "\n",
    "# Output for debugging\n",
    "print(f\"Chosen distribution: {dist.value}\")\n",
    "print(f\"Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429868bf-6f9f-46d9-bba1-e368c895637b",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d9d8b5b-5d6f-475b-a706-08d80e849f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration = 0.5332407999994757\n",
      "Average Loss = 1996.4408653942141\n",
      "Frequency = 4.00696\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "threshold = 1000\n",
    "\n",
    "frequency_input = DistributionInput(\n",
    "    dist=DistributionType.POISSON,\n",
    "    params=[4],\n",
    ")\n",
    "\n",
    "severity_input = DistributionInput(\n",
    "    dist=DistributionType.PARETO,\n",
    "    params=[2],\n",
    ")\n",
    "\n",
    "simulated_years = 100_000\n",
    "modelfile_id = 1\n",
    "start = time.perf_counter()\n",
    "modelyearloss = get_modelyearloss_frequency_severity(\n",
    "    threshold,\n",
    "    frequency_input,\n",
    "    severity_input,\n",
    "    simulated_years,\n",
    "    modelfile_id,\n",
    ")\n",
    "print(f\"Duration = {time.perf_counter() - start}\")\n",
    "print(f\"Average Loss = {modelyearloss['loss'].mean()}\")\n",
    "print(f\"Frequency = {len(modelyearloss['loss']) / simulated_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d83f9bf0-9656-4b09-a11c-3ac644386563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (400_696, 5)\n",
      "┌────────┬─────┬────────┬───────────┬──────────────┐\n",
      "│ year   ┆ day ┆ loss   ┆ loss_type ┆ modelfile_id │\n",
      "│ ---    ┆ --- ┆ ---    ┆ ---       ┆ ---          │\n",
      "│ i64    ┆ i32 ┆ f64    ┆ str       ┆ i64          │\n",
      "╞════════╪═════╪════════╪═══════════╪══════════════╡\n",
      "│ 1      ┆ 167 ┆ 1165.0 ┆ non_cat   ┆ 1            │\n",
      "│ 1      ┆ 327 ┆ 1691.0 ┆ cat       ┆ 1            │\n",
      "│ 1      ┆ 359 ┆ 1152.0 ┆ non_cat   ┆ 1            │\n",
      "│ 1      ┆ 169 ┆ 1099.0 ┆ cat       ┆ 1            │\n",
      "│ 1      ┆ 66  ┆ 1041.0 ┆ non_cat   ┆ 1            │\n",
      "│ …      ┆ …   ┆ …      ┆ …         ┆ …            │\n",
      "│ 100000 ┆ 212 ┆ 1354.0 ┆ non_cat   ┆ 1            │\n",
      "│ 100000 ┆ 362 ┆ 1136.0 ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 44  ┆ 1136.0 ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 92  ┆ 9173.0 ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 48  ┆ 1083.0 ┆ cat       ┆ 1            │\n",
      "└────────┴─────┴────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(modelyearloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846cfc-7582-4e2d-9e4d-0c700173f432",
   "metadata": {},
   "source": [
    "## Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65c16-cc0e-422f-9e1c-814d0e028952",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa3ca2d7-9b01-49a0-9eb9-17a2ac38dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sqlalchemy import desc, insert, select, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from database import (\n",
    "    Analysis,\n",
    "    Client,\n",
    "    HistoLossFile,\n",
    "    ModelFile,\n",
    "    ModelYearLoss,\n",
    "    FrequencyModel,\n",
    "    SeverityModel,\n",
    "    FrequencySeverityModel,\n",
    "    session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98ebb8-59f9-4f83-9997-60d5b5f0c1ba",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f06779b1-2cfc-440e-a796-68faed7d1994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 'AXA' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a client\n",
    "def create_client(session, client_name):\n",
    "    \"\"\"\n",
    "    Adds a new client to the database.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        client_name (str): The name of the client to be added.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a new client\n",
    "        client = Client(name=client_name)\n",
    "\n",
    "        # Add the client to the session\n",
    "        session.add(client)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(f\"Client '{client_name}' added successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "create_client(session, client_name=\"AXA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdccaa70-36f4-4c0a-8145-43e0e2f6a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create an analysis and associate it with a client\n",
    "def add_analysis_to_client(session, client_id):\n",
    "    \"\"\"\n",
    "    Creates an analysis and associates it with a client.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        client_id (int): The ID of the client to associate the analysis with.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the client\n",
    "        client = session.get_one(Client, client_id)\n",
    "\n",
    "        # Create a new analysis\n",
    "        analysis = Analysis()\n",
    "\n",
    "        # Associate the analysis with the client\n",
    "        client.analyses.append(analysis)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(\"Analysis added successfully\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "add_analysis_to_client(session, client_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c6fcaff-1103-438d-a0de-0202fc93e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical loss file added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create a historical loss file and associate it with a client and an analysis\n",
    "def create_historical_loss_file(session, analysis_id):\n",
    "    \"\"\"\n",
    "    Creates a historical loss file and associates it with a client and an analysis.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        analysis_id (int): The ID of the analysis to associate the historical loss file with.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the analysis and the associated client\n",
    "        analysis = session.get(Analysis, analysis_id)\n",
    "        if not analysis:\n",
    "            raise ValueError(f\"Analysis with ID {analysis_id} not found.\")\n",
    "        client = analysis.client\n",
    "\n",
    "        # Create the historical loss file\n",
    "        histolossfile = HistoLossFile()\n",
    "\n",
    "        # Associate the historical loss file with the client and analysis\n",
    "        client.histolossfiles.append(histolossfile)\n",
    "        analysis.histolossfiles.append(histolossfile)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(\"Historical loss file added successfully\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "create_historical_loss_file(session, analysis_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34b984f2-74c8-4c0e-9220-34dcb6865016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency-severity loss model\n",
    "def create_frequency_severity_model(\n",
    "    session: Session,\n",
    "    analysis_id: int,\n",
    "    lossfile_id: int,\n",
    "    threshold: float,\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    years_simulated: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a frequency-severity model and persists related data in the database.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use for database operations.\n",
    "        analysis_id (int): ID of the analysis to associate the model with.\n",
    "        lossfile_id (int): ID of the loss file to associate the model with.\n",
    "        threshold (float): Threshold parameter for the model.\n",
    "        frequency_input (DistributionInput): Input parameters for the frequency model.\n",
    "        severity_input (DistributionInput): Input parameters for the severity model.\n",
    "        years_simulated (int): Number of years simulated for the model.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs during the process.\n",
    "        Exception: If an unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch analysis and ensure it exists\n",
    "        analysis = session.get(Analysis, analysis_id)\n",
    "        if not analysis:\n",
    "            raise ValueError(f\"Analysis with ID {analysis_id} not found.\")\n",
    "        client_id = analysis.client_id\n",
    "\n",
    "        # Create frequency and severity models\n",
    "        start_time = time.perf_counter()\n",
    "        frequencymodel = FrequencyModel(\n",
    "            **{\n",
    "                f\"parameter_{i}\": param\n",
    "                for i, param in enumerate(frequency_input.params)\n",
    "            }\n",
    "        )\n",
    "        severitymodel = SeverityModel(\n",
    "            **{f\"parameter_{i}\": param for i, param in enumerate(severity_input.params)}\n",
    "        )\n",
    "\n",
    "        # Create the frequency-severity model\n",
    "        modelfile = FrequencySeverityModel(\n",
    "            model_type=\"frequencyseveritymodel\",\n",
    "            threshold=threshold,\n",
    "            years_simulated=years_simulated,\n",
    "            lossfile_id=lossfile_id,\n",
    "            frequencymodel=frequencymodel,\n",
    "            severitymodel=severitymodel,\n",
    "        )\n",
    "\n",
    "        # Link the model to the analysis and the client\n",
    "        analysis.client.modelfiles.append(modelfile)\n",
    "        analysis.modelfiles.append(modelfile)\n",
    "        print(\n",
    "            f\"Time to create the model file in the session: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Flush to get modelfile ID\n",
    "        start_time = time.perf_counter()\n",
    "        session.flush()\n",
    "        print(\n",
    "            f\"Time to flush the session: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Generate year loss data\n",
    "        start_time = time.perf_counter()\n",
    "        modelyearloss = get_modelyearloss_frequency_severity(\n",
    "            threshold, frequency_input, severity_input, years_simulated, modelfile.id\n",
    "        )\n",
    "        print(modelyearloss)\n",
    "        print(\n",
    "            f\"Time to generate model year loss: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Insert records into the database\n",
    "        start_time = time.perf_counter()\n",
    "        session.execute(insert(ModelYearLoss), modelyearloss.to_dicts())\n",
    "        print(\n",
    "            f\"Time to insert model year loss records: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Commit the transaction\n",
    "        start_time = time.perf_counter()\n",
    "        session.commit()\n",
    "        print(\n",
    "            f\"Time to commit transaction: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e75de26d-c2f5-4b3c-909a-55954c478327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create the model file in the session: 0.01 seconds\n",
      "Time to flush the session: 0.00 seconds\n",
      "shape: (200_045, 5)\n",
      "┌────────┬─────┬────────┬───────────┬──────────────┐\n",
      "│ year   ┆ day ┆ loss   ┆ loss_type ┆ modelfile_id │\n",
      "│ ---    ┆ --- ┆ ---    ┆ ---       ┆ ---          │\n",
      "│ i64    ┆ i32 ┆ f64    ┆ str       ┆ i64          │\n",
      "╞════════╪═════╪════════╪═══════════╪══════════════╡\n",
      "│ 1      ┆ 256 ┆ 1150.0 ┆ cat       ┆ 58           │\n",
      "│ 2      ┆ 275 ┆ 1715.0 ┆ non_cat   ┆ 58           │\n",
      "│ 3      ┆ 364 ┆ 2509.0 ┆ non_cat   ┆ 58           │\n",
      "│ 4      ┆ 259 ┆ 2471.0 ┆ cat       ┆ 58           │\n",
      "│ 4      ┆ 20  ┆ 1020.0 ┆ cat       ┆ 58           │\n",
      "│ …      ┆ …   ┆ …      ┆ …         ┆ …            │\n",
      "│ 99994  ┆ 270 ┆ 1097.0 ┆ cat       ┆ 58           │\n",
      "│ 99996  ┆ 165 ┆ 1031.0 ┆ cat       ┆ 58           │\n",
      "│ 99996  ┆ 4   ┆ 2792.0 ┆ non_cat   ┆ 58           │\n",
      "│ 99999  ┆ 98  ┆ 1285.0 ┆ non_cat   ┆ 58           │\n",
      "│ 100000 ┆ 323 ┆ 1165.0 ┆ cat       ┆ 58           │\n",
      "└────────┴─────┴────────┴───────────┴──────────────┘\n",
      "Time to generate model year loss: 0.34 seconds\n",
      "Time to add the modelfile_id: 0.00 seconds\n",
      "Time to insert model year loss records: 11.48 seconds\n",
      "Time to commit transaction: 0.00 seconds\n",
      "Total Duration = 11.842117600001075\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "start = time.perf_counter()\n",
    "create_frequency_severity_model(\n",
    "    session,\n",
    "    analysis_id=1,\n",
    "    lossfile_id=1,\n",
    "    threshold=1000,\n",
    "    frequency_input=DistributionInput(\n",
    "        dist=DistributionType.POISSON,\n",
    "        params=[2, 0, 0, 0, 0],\n",
    "    ),\n",
    "    severity_input=DistributionInput(\n",
    "        dist=DistributionType.PARETO,\n",
    "        params=[2, 0, 0, 0, 0],\n",
    "    ),\n",
    "    years_simulated=100_000,\n",
    ")\n",
    "\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Total Duration = {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "721696d9-3953-4ac5-b0b6-51f07fc1b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this cell\n",
    "query = select(ModelFile).order_by(desc(ModelFile.id)).limit(1)\n",
    "last_record = session.execute(query).scalars().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fba8266c-5e8d-4b14-b4f3-7500937c97c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0.00010379999912402127\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "last_record.yearlosses\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Duration: {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45bc9f3b-aa6e-4009-ae3a-5b39aaee821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The historical loss file has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete a historical loss file\n",
    "\n",
    "# Input data\n",
    "histlossfile_id = 3\n",
    "\n",
    "try:\n",
    "    histolossfile = session.get(HistoLossFile, histlossfile_id)\n",
    "    if not histolossfile:\n",
    "        raise ValueError(f\"Historical loss file with ID {histlossfile_id} not found.\")\n",
    "    session.delete(histolossfile)\n",
    "    session.commit()\n",
    "    print(f\"The historical loss file has been deleted.\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    session.rollback()\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An unexpected error occured: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c99b32e-7e13-4f31-a694-760b0e24adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm = session.scalars(select(FrequencySeverityModel)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40da31df-8625-40b9-9ffa-0a0e19d9584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrequencySeverityModel(id=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01d037d5-4c79-47eb-9cdd-072a18231ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.delete(fsm)\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An error occured: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9089e6-e918-47e4-94cf-0560d2325a4e",
   "metadata": {},
   "source": [
    "# DELETE A FREQUENCY SEVERITY MODEL, A FREQUENCY SOLO, A SEVERITY SOLO\n",
    "\n",
    "# TRANSFORM SCRIPTS INTO FUNCTIONS\n",
    "\n",
    "# THEN CORRECT DAYS RECORDING IN THE DATABASE\n",
    "\n",
    "# CORRECT IMPLEMENTATION OF THRESHOLD FOR GENERATION LOSSES\n",
    "\n",
    "# THEN CREATE JIRA SPECIFIC ISSUES\n",
    "\n",
    "# USE ASYNCIO??? (ASK CHATGPT)\n",
    "\n",
    "# THEN WORKSHOP WITH ANTOINE B TO REVIEW CHANGES LIKE THOSE IN PYDANTIC FOR FREQUENCYINPUT SEVERITYINPUT ETC\n",
    "\n",
    "# THEN DO UI FOR TRAIN_SQLA FOR  ENGINE ONLY FOR ACTUARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d4b84-481c-4ab5-87a4-9eda08108598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153442c4-d952-48af-a3bd-c598cc0cc6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
