{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fac031a-df9e-4e09-a534-696712aecf8f",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://flask-sqlalchemy.readthedocs.io/en/stable/\n",
    "* https://docs.sqlalchemy.org/en/20/orm/inheritance.html#concrete-table-inheritance\n",
    "* https://docs.sqlalchemy.org/en/20/_modules/examples/performance/bulk_inserts.html\n",
    "* https://docs.sqlalchemy.org/en/20/orm/large_collections.html#bulk-insert-of-new-items\n",
    "* https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549210ef-e66e-4461-9dd3-7a1a1f1d04f7",
   "metadata": {},
   "source": [
    "## Refactoring notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b18cfc-7e7e-431a-847f-2765019ef6c4",
   "metadata": {},
   "source": [
    "- The relationship between a frequency severity model and the input premium file is missing\n",
    "- The back-relationship were missing for handling properly session.delete(histolossfile) :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12788cb3-ca8f-4534-97b2-4ea3fca4e51b",
   "metadata": {},
   "source": [
    "```\n",
    "class Analysis(CommonMixin, Base):\n",
    "    \"\"\"Represents an analysis entity.\"\"\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "    client: Mapped[\"Client\"] = relationship(back_populates=\"analyses\")\n",
    "\n",
    "    histolossfiles: Mapped[List[\"HistoLossFile\"]] = relationship(\n",
    "        secondary=lambda: analysis_histolossfile_table, back_populates=\"analyses\"\n",
    "    )\n",
    "    modelfiles: Mapped[List[\"ModelFile\"]] = relationship(\n",
    "        secondary=lambda: analysis_modelfile_table, back_populates=\"analyses\"\n",
    "    )\n",
    "\n",
    "    class HistoLossFile(CommonMixin, Base):\n",
    "\n",
    "    analyses: Mapped[List[Analysis]] = relationship(\n",
    "        secondary=lambda: analysis_histolossfile_table, back_populates=\"histolossfiles\"\n",
    "    )\n",
    "\n",
    "  class ModelFile(CommonMixin, Base):\n",
    "        \"\"\"Base class for model files.\"\"\"\n",
    "    \n",
    "        id: Mapped[int] = mapped_column(primary_key=True)\n",
    "        model_type: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "        years_simulated: Mapped[int] = mapped_column(nullable=False)\n",
    "    \n",
    "        client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "        client: Mapped[\"Client\"] = relationship(back_populates=\"modelfiles\")\n",
    "    \n",
    "        yearlosses: Mapped[List[\"ModelYearLoss\"]] = relationship(\n",
    "            back_populates=\"modelfile\",\n",
    "            cascade=\"all, delete-orphan\",\n",
    "        )\n",
    "    \n",
    "        analyses: Mapped[List[Analysis]] = relationship(\n",
    "            secondary=lambda: analysis_modelfile_table, back_populates=\"modelfiles\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- Cascade delete, all for client-analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff1d73-22cd-4002-ba5e-5df747f6a353",
   "metadata": {},
   "source": [
    "```\n",
    "class Client(CommonMixin, Base): \"\"\"Represents a client entity.\"\"\"\n",
    "\n",
    "  id: Mapped[int] = mapped_column(primary_key=True)\n",
    "  name: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "\n",
    "  analyses: Mapped[List[\"Analysis\"]] = relationship(\n",
    "      back_populates=\"client\", cascade=\"all, delete-orphan\"\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162c4ef-7963-421d-98ee-8eb5d82d2283",
   "metadata": {},
   "source": [
    "- The Pydantic classes FrequencyInput and SeverityInput need to be reviewed and refactored\n",
    "- The attribute treshold is missing in the FrequencyModel class\n",
    "- Start frequency and severity models parameters with index 0\n",
    "- The threshold of frequency and severity models is that of the related frequency_severity_model => Remove attribute threshold from severity model\n",
    "- Use ModelType in polymorphic identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54af16-f6a3-49d4-b4c2-e86a16d8329c",
   "metadata": {},
   "source": [
    "- Confusion entre input et output :\n",
    "- https://gitlab.com/ccr-re-df/products/app/backends/tarification-nonvie-backend/-/blob/dev/app/api/routes/model.py?ref_type=heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6269ea9-27b7-4683-98f8-536aaf5d372d",
   "metadata": {},
   "source": [
    "```\n",
    "async def generate_stochastic_year_loss_table_and_metadata(\n",
    "    frequence_model_input: FrequencyModelOutput,\n",
    "    severity_model_input: SeverityModelOutput,\n",
    "    frequence_severity_model_input: FrequencySeverityModelInputExtend,\n",
    "    threshold_input: float,\n",
    "    analysis_id: int,\n",
    "    user_id: int,\n",
    "    lossfile_id: int,\n",
    "    injected_model_service: ModelServiceDep,\n",
    ") -> StochasticModelRouteResponse:\n",
    "    \"\"\"\n",
    "    Generate stochastic year loss table along with metadata.\n",
    "\n",
    "    Args:\n",
    "        frequence_model_input (FrequencyModelOutput): Output from the frequency model.\n",
    "        severity_model_input (SeverityModelOutput): Output from the severity model.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd4bb3-3492-4dd5-95f9-bf2380809997",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd7511-27f7-4d3a-bbd8-464a1aa82cea",
   "metadata": {},
   "source": [
    "### Enumerations and Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df406879-7e64-4964-b0c1-52e604df0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ModelType(Enum):\n",
    "    \"\"\"Defines the supported loss models.\"\"\"\n",
    "\n",
    "    EMPIRICAL = \"empirical\"\n",
    "    # FREQUENCY_SEVERITY = \"frequency_severity\"\n",
    "    FREQUENCY_SEVERITY = \"frequencyseveritymodel\"\n",
    "    COMPOSITE_FREQUENCY_SEVERITY = \"composite_frequency_severity\"\n",
    "    EXPOSURE_BASED = \"exposure_based\"\n",
    "\n",
    "\n",
    "class DistributionType(Enum):\n",
    "    \"\"\"Defines the supported statistical distributions.\"\"\"\n",
    "\n",
    "    POISSON = \"poisson\"\n",
    "    NEGATIVE_BINOMIAL = \"negative_binomial\"\n",
    "    PARETO = \"pareto\"\n",
    "\n",
    "\n",
    "class LossType(Enum):\n",
    "    \"\"\"Defines the loss types.\"\"\"\n",
    "\n",
    "    CAT = \"cat\"\n",
    "    NON_CAT = \"non_cat\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DistributionInput:\n",
    "    \"\"\"\n",
    "    Configuration for a statistical distribution.\n",
    "\n",
    "    Attributes:\n",
    "        dist: The distribution type (enum).\n",
    "        params: Parameters specific to the distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    dist: DistributionType\n",
    "    params: list[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b719d63-a33c-486c-9310-942158abbf19",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ecd867-201c-4729-8d42-2bfa52208eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen distribution: DistributionType.PARETO\n",
      "Parameters: [1000, 2]\n",
      "Mean draw: 1993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.stats import nbinom, pareto, poisson\n",
    "\n",
    "\n",
    "# Main function to generate model year loss\n",
    "def get_modelyearloss_frequency_severity(\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    simulated_years: int,\n",
    "    modelfile_id: int,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate loss data for a frequency-severity model over a number of simulated years.\n",
    "\n",
    "    Args:\n",
    "        frequency_input (DistributionInput): Distribution defining the frequency of events per year.\n",
    "        severity_input (DistributionInput): Distribution defining the severity of each event.\n",
    "        simulated_years (int): Number of years to simulate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"year\": List of years for each loss event.\n",
    "            - \"day\": Random day of the year for each loss.\n",
    "            - \"loss\": Calculated loss values.\n",
    "            - \"loss_type\": Type of loss (e.g., catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    frequencies = generate_frequencies(frequency_input, simulated_years)\n",
    "    loss_count = frequencies.sum()\n",
    "    years = generate_years(frequencies)\n",
    "    days = generate_days(loss_count)\n",
    "    losses = generate_losses_from_parametric_dist(severity_input, loss_count)\n",
    "    loss_types = generate_loss_types(loss_count)\n",
    "    modelfile_ids = np.repeat(modelfile_id, loss_count)\n",
    "\n",
    "    modelyearloss = pl.DataFrame(\n",
    "        {\n",
    "            \"year\": years,\n",
    "            \"day\": days,\n",
    "            \"loss\": losses,\n",
    "            \"loss_type\": loss_types,\n",
    "            \"modelfile_id\": modelfile_ids,\n",
    "        }\n",
    "    )\n",
    "    return modelyearloss\n",
    "\n",
    "\n",
    "def generate_frequencies(\n",
    "    frequency_input: DistributionInput,\n",
    "    size: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a list of event frequencies based on a specified distribution.\n",
    "\n",
    "    Args:\n",
    "        frequency_input (DistributionInput): The distribution and parameters for frequency generation.\n",
    "        size (int): Number of values to generate.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of event frequencies for each simulated year.\n",
    "    \"\"\"\n",
    "    frequencies = get_sample_from_dist(frequency_input, size)\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def generate_years(frequencies: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate a list of years for loss events based on event frequencies.\n",
    "\n",
    "    Args:\n",
    "        frequencies (list[int]): A list where each element represents the number of events in a year.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of years, repeated according to their respective frequencies.\n",
    "    \"\"\"\n",
    "    years = [\n",
    "        year for year, freq in enumerate(frequencies, start=1) for _ in range(freq)\n",
    "    ]\n",
    "    return years\n",
    "\n",
    "\n",
    "def generate_days(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random days of the year for loss events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of days to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of random integers representing days (1 to 365).\n",
    "    \"\"\"\n",
    "    days = np.random.randint(1, 366, size)\n",
    "    return days\n",
    "\n",
    "\n",
    "def generate_loss_types(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random loss types (catastrophic or non-catastrophic) for events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of loss types to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of randomly chosen loss types (catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    loss_types = np.random.choice([LossType.CAT.value, LossType.NON_CAT.value], size)\n",
    "    return loss_types\n",
    "\n",
    "\n",
    "def generate_losses_from_parametric_dist(\n",
    "    severity_input: DistributionInput,\n",
    "    loss_count: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a set of rounded loss values from a parametric distribution.\n",
    "\n",
    "    Args:\n",
    "        severity_input (DistributionInput): Parameters defining the severity distribution.\n",
    "        loss_count (int): The number of loss values to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of rounded loss values.\n",
    "    \"\"\"\n",
    "    sample = get_sample_from_dist(severity_input, loss_count)\n",
    "    sample_to_int = sample.astype(int)\n",
    "    return sample_to_int\n",
    "\n",
    "\n",
    "def get_sample_from_dist(\n",
    "    distribution_input: DistributionInput, size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a sample from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution_input (DistributionInput): An object containing the distribution type\n",
    "            (e.g., Poisson, Negative Binomial, Pareto) and its associated parameters.\n",
    "        size (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of samples drawn from the specified distribution.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the distribution type is not supported.\n",
    "    \"\"\"\n",
    "    dist = distribution_input.dist\n",
    "    params = distribution_input.params\n",
    "\n",
    "    match dist:\n",
    "        case DistributionType.POISSON:\n",
    "            return poisson.rvs(mu=params[0], size=size)\n",
    "        case DistributionType.NEGATIVE_BINOMIAL:\n",
    "            return nbinom.rvs(n=params[0], p=params[1], size=size)\n",
    "        case DistributionType.PARETO:\n",
    "            return pareto.rvs(scale=params[0], b=params[1], size=size)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported distribution: {dist}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "distribution_input = DistributionInput(dist=DistributionType.PARETO, params=[1000, 2])\n",
    "sample = get_sample_from_dist(distribution_input, size=1_000_000)\n",
    "\n",
    "# Output for debugging\n",
    "print(f\"Chosen distribution: {distribution_input.dist}\")\n",
    "print(f\"Parameters: {distribution_input.params}\")\n",
    "print(f\"Mean draw: {sample.mean().astype(int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429868bf-6f9f-46d9-bba1-e368c895637b",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9d8b5b-5d6f-475b-a706-08d80e849f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration = 0.30316780001157895\n",
      "Average Loss = 2002.0331773958185\n",
      "Frequency = 3.99519\n",
      "shape: (399_519, 5)\n",
      "┌────────┬─────┬───────┬───────────┬──────────────┐\n",
      "│ year   ┆ day ┆ loss  ┆ loss_type ┆ modelfile_id │\n",
      "│ ---    ┆ --- ┆ ---   ┆ ---       ┆ ---          │\n",
      "│ i64    ┆ i32 ┆ i64   ┆ str       ┆ i64          │\n",
      "╞════════╪═════╪═══════╪═══════════╪══════════════╡\n",
      "│ 1      ┆ 172 ┆ 1887  ┆ cat       ┆ 1            │\n",
      "│ 1      ┆ 188 ┆ 1289  ┆ cat       ┆ 1            │\n",
      "│ 1      ┆ 43  ┆ 1118  ┆ non_cat   ┆ 1            │\n",
      "│ 2      ┆ 250 ┆ 1938  ┆ non_cat   ┆ 1            │\n",
      "│ 2      ┆ 74  ┆ 3093  ┆ cat       ┆ 1            │\n",
      "│ …      ┆ …   ┆ …     ┆ …         ┆ …            │\n",
      "│ 99999  ┆ 224 ┆ 2022  ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 220 ┆ 1991  ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 350 ┆ 1650  ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 270 ┆ 1178  ┆ cat       ┆ 1            │\n",
      "│ 100000 ┆ 231 ┆ 19950 ┆ cat       ┆ 1            │\n",
      "└────────┴─────┴───────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "frequency_input = DistributionInput(\n",
    "    dist=DistributionType.POISSON,\n",
    "    params=[4],\n",
    ")\n",
    "\n",
    "severity_input = DistributionInput(\n",
    "    dist=DistributionType.PARETO,\n",
    "    params=[1000, 2],\n",
    ")\n",
    "\n",
    "simulated_years = 100_000\n",
    "modelfile_id = 1\n",
    "start = time.perf_counter()\n",
    "modelyearloss = get_modelyearloss_frequency_severity(\n",
    "    frequency_input,\n",
    "    severity_input,\n",
    "    simulated_years,\n",
    "    modelfile_id,\n",
    ")\n",
    "print(f\"Duration = {time.perf_counter() - start}\")\n",
    "print(f\"Average Loss = {modelyearloss[\"loss\"].mean()}\")\n",
    "print(f\"Frequency = {len(modelyearloss[\"loss\"]) / simulated_years}\")\n",
    "print(modelyearloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846cfc-7582-4e2d-9e4d-0c700173f432",
   "metadata": {},
   "source": [
    "## Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65c16-cc0e-422f-9e1c-814d0e028952",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fa3ca2d7-9b01-49a0-9eb9-17a2ac38dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Type\n",
    "\n",
    "from sqlalchemy import desc, insert, select, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import DeclarativeMeta, Session\n",
    "\n",
    "from database import (\n",
    "    Analysis,\n",
    "    Client,\n",
    "    FrequencyModel,\n",
    "    FrequencySeverityModel,\n",
    "    HistoLossFile,\n",
    "    ModelFile,\n",
    "    ModelYearLoss,\n",
    "    SeverityModel,\n",
    "    session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98ebb8-59f9-4f83-9997-60d5b5f0c1ba",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f06779b1-2cfc-440e-a796-68faed7d1994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 'AXA' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a client\n",
    "def add_client(session, client_name):\n",
    "    \"\"\"\n",
    "    Add a new client to the database.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        client_name (str): The name of the client to be added.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a new client\n",
    "        client = Client(name=client_name)\n",
    "\n",
    "        # Add the client to the session\n",
    "        session.add(client)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(f\"Client '{client_name}' added successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cdccaa70-36f4-4c0a-8145-43e0e2f6a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an analysis and associate it with a client\n",
    "def add_analysis_to_client(session, client_id):\n",
    "    \"\"\"\n",
    "    Create an analysis and associates it with a client.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        client_id (int): The ID of the client to associate the analysis with.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the client\n",
    "        client = session.get_one(Client, client_id)\n",
    "\n",
    "        # Create a new analysis\n",
    "        analysis = Analysis()\n",
    "\n",
    "        # Associate the analysis with the client\n",
    "        client.analyses.append(analysis)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(\"Analysis added successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2c6fcaff-1103-438d-a0de-0202fc93e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a historical loss file and associate it with a client and an analysis\n",
    "def create_historical_loss_file(session: Session, analysis_id: int):\n",
    "    \"\"\"\n",
    "    Create a historical loss file and associates it with a client and an analysis.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use.\n",
    "        analysis_id (int): The ID of the analysis to associate the historical loss file with.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: If any other unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the analysis and the associated client\n",
    "        analysis = session.get(Analysis, analysis_id)\n",
    "        if not analysis:\n",
    "            raise ValueError(f\"Analysis with ID {analysis_id} not found.\")\n",
    "        client = analysis.client\n",
    "\n",
    "        # Create the historical loss file\n",
    "        histolossfile = HistoLossFile()\n",
    "\n",
    "        # Associate the historical loss file with the client and analysis\n",
    "        client.histolossfiles.append(histolossfile)\n",
    "        analysis.histolossfiles.append(histolossfile)\n",
    "\n",
    "        # Commit the transaction\n",
    "        session.commit()\n",
    "        print(\"Historical loss file added successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "34b984f2-74c8-4c0e-9220-34dcb6865016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency-severity loss model\n",
    "def create_frequency_severity_model(\n",
    "    session: Session,\n",
    "    analysis_id: int,\n",
    "    lossfile_id: int,\n",
    "    threshold: float,\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    years_simulated: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a frequency-severity model and persists related data in the database.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use for database operations.\n",
    "        analysis_id (int): ID of the analysis to associate the model with.\n",
    "        lossfile_id (int): ID of the loss file to associate the model with.\n",
    "        threshold (float): Threshold parameter for the model.\n",
    "        frequency_input (DistributionInput): Input parameters for the frequency model.\n",
    "        severity_input (DistributionInput): Input parameters for the severity model.\n",
    "        years_simulated (int): Number of years simulated for the model.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs during the process.\n",
    "        Exception: If an unexpected error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch analysis and ensure it exists\n",
    "        analysis = session.get(Analysis, analysis_id)\n",
    "        if not analysis:\n",
    "            raise ValueError(f\"Analysis with ID {analysis_id} not found.\")\n",
    "        client_id = analysis.client_id\n",
    "\n",
    "        # Create frequency and severity models\n",
    "        start_time = time.perf_counter()\n",
    "        frequencymodel = FrequencyModel(\n",
    "            **{\n",
    "                f\"parameter_{i}\": param\n",
    "                for i, param in enumerate(frequency_input.params)\n",
    "            }\n",
    "        )\n",
    "        severitymodel = SeverityModel(\n",
    "            **{f\"parameter_{i}\": param for i, param in enumerate(severity_input.params)}\n",
    "        )\n",
    "\n",
    "        # Create the frequency-severity model\n",
    "        modelfile = FrequencySeverityModel(\n",
    "            model_type=\"frequencyseveritymodel\",\n",
    "            threshold=threshold,\n",
    "            years_simulated=years_simulated,\n",
    "            lossfile_id=lossfile_id,\n",
    "            frequencymodel=frequencymodel,\n",
    "            severitymodel=severitymodel,\n",
    "        )\n",
    "\n",
    "        # Link the model to the analysis and the client\n",
    "        analysis.client.modelfiles.append(modelfile)\n",
    "        analysis.modelfiles.append(modelfile)\n",
    "        print(\n",
    "            f\"Time to create model file: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Flush to get modelfile ID\n",
    "        start_time = time.perf_counter()\n",
    "        session.flush()\n",
    "        modelfile_id = modelfile.id\n",
    "        print(\n",
    "            f\"Time to flush the session: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Generate year loss data\n",
    "        start_time = time.perf_counter()\n",
    "        modelyearloss = get_modelyearloss_frequency_severity(\n",
    "            frequency_input, severity_input, years_simulated, modelfile_id\n",
    "        )\n",
    "        print(\n",
    "            f\"Time to generate year loss data: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Insert records into the database\n",
    "        start_time = time.perf_counter()\n",
    "        session.execute(insert(ModelYearLoss), modelyearloss.to_dicts())\n",
    "        print(\n",
    "            f\"Time to insert year loss records into database: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Commit the transaction\n",
    "        start_time = time.perf_counter()\n",
    "        session.commit()\n",
    "        print(\n",
    "            f\"Time to commit transaction: {time.perf_counter() - start_time:.2f} seconds\"\n",
    "        )\n",
    "        print(\"Frequency-Severity Model created successfully.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "45bc9f3b-aa6e-4009-ae3a-5b39aaee821a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete a database record\n",
    "def delete_db_record(\n",
    "    session: Session,\n",
    "    model: Type[DeclarativeMeta],\n",
    "    record_id: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Delete a record from the database by its model class and ID.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use for database operations.\n",
    "        model (Type[DeclarativeMeta]): The SQLAlchemy model class of the record to delete.\n",
    "        record_id (int): The ID of the record to delete.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the record with the given ID is not found.\n",
    "        SQLAlchemyError: If a database error occurs.\n",
    "        Exception: For any other unexpected errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the record\n",
    "        record = session.get(model, record_id)\n",
    "        if not record:\n",
    "            raise ValueError(f\"{model.__name__} record with ID {record_id} not found.\")\n",
    "\n",
    "        # Delete the record\n",
    "        session.delete(record)\n",
    "        session.commit()\n",
    "        print(f\"{model.__name__} record with ID {record_id} has been deleted.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred while deleting {model.__name__} record: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(\n",
    "            f\"An unexpected error occurred while deleting {model.__name__} record: {e}\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "13d3c58f-6cb1-4f42-9c2c-1e4a4feab07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 'AXA' added successfully.\n",
      "Analysis added successfully.\n",
      "Historical loss file added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a client, an analysis and a historical loss file\n",
    "add_client(session, client_name=\"AXA\")\n",
    "add_analysis_to_client(session, client_id=1)\n",
    "create_historical_loss_file(session, analysis_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "396fc813-c224-4469-b374-3e9c7d6d3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create model file: 0.01 seconds\n",
      "Time to flush the session: 0.00 seconds\n",
      "Time to generate year loss data: 0.03 seconds\n",
      "Time to insert year loss records into database: 2.18 seconds\n",
      "Time to commit transaction: 0.00 seconds\n",
      "Frequency-Severity Model created successfully.\n",
      "Total Duration = 2.232603600001312\n"
     ]
    }
   ],
   "source": [
    "# Create a frequency-severity model\n",
    "start = time.perf_counter()\n",
    "create_frequency_severity_model(\n",
    "    session,\n",
    "    analysis_id=1,\n",
    "    lossfile_id=1,\n",
    "    threshold=1000,\n",
    "    frequency_input=DistributionInput(\n",
    "        dist=DistributionType.POISSON,\n",
    "        params=[3, 0, 0, 0, 0],\n",
    "    ),\n",
    "    severity_input=DistributionInput(\n",
    "        dist=DistributionType.PARETO,\n",
    "        params=[1000, 2, 0, 0, 0],\n",
    "    ),\n",
    "    years_simulated=10_000,\n",
    ")\n",
    "\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Total Duration = {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "648a05f5-94f1-4425-85d3-c0b97e9e2202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client record with ID 34 has been deleted.\n"
     ]
    }
   ],
   "source": [
    "delete_db_record(session, Client, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c99b32e-7e13-4f31-a694-760b0e24adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = session.scalars(select(FrequencyModel)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40da31df-8625-40b9-9ffa-0a0e19d9584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrequencyModel(id=4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01d037d5-4c79-47eb-9cdd-072a18231ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.delete(fm)\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An error occured: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9089e6-e918-47e4-94cf-0560d2325a4e",
   "metadata": {},
   "source": [
    "# ADD ASYNCIO??? (ASK CHATGPT)\n",
    "\n",
    "# ADD PREMIUM FILE ID\n",
    "\n",
    "# FIX THE PROBLEM WITH DELETING A FREQUENCY SOLO, A SEVERITY SOLO\n",
    "\n",
    "# THEN CREATE JIRA SPECIFIC ISSUES\n",
    "\n",
    "# THEN WORKSHOP WITH ANTOINE B TO REVIEW CHANGES LIKE THOSE IN PYDANTIC FOR FREQUENCYINPUT SEVERITYINPUT ETC\n",
    "\n",
    "# THEN DO UI FOR TRAIN_SQLA FOR  ENGINE ONLY FOR ACTUARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d4b84-481c-4ab5-87a4-9eda08108598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153442c4-d952-48af-a3bd-c598cc0cc6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
