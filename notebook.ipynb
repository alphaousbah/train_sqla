{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fac031a-df9e-4e09-a534-696712aecf8f",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://flask-sqlalchemy.readthedocs.io/en/stable/\n",
    "* https://docs.sqlalchemy.org/en/20/orm/inheritance.html#concrete-table-inheritance\n",
    "* https://docs.sqlalchemy.org/en/20/_modules/examples/performance/bulk_inserts.html\n",
    "* https://docs.sqlalchemy.org/en/20/orm/large_collections.html#bulk-insert-of-new-items\n",
    "* https://www.restack.io/p/adding-columns-sqlalchemy-models-answer-async-bulk-insert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549210ef-e66e-4461-9dd3-7a1a1f1d04f7",
   "metadata": {},
   "source": [
    "## Refactoring notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a5368-e679-4afb-bc7a-d6bc7966109b",
   "metadata": {},
   "source": [
    "- The relationship between a frequency severity model and the input premium file is missing\n",
    "- The back-relationship were missing for handling properly session.delete(histolossfile) :\n",
    "\n",
    "-\n",
    "\n",
    "    class Analysis(CommonMixin, Base):\n",
    "        \"\"\"Represents an analysis entity.\"\"\"\n",
    "    \n",
    "        id: Mapped[int] = mapped_column(primary_key=True)\n",
    "        client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "        client: Mapped[\"Client\"] = relationship(back_populates=\"analyses\")\n",
    "    \n",
    "        histolossfiles: Mapped[List[\"HistoLossFile\"]] = relationship(\n",
    "            secondary=lambda: analysis_histolossfile_table, back_populates=\"analyses\"\n",
    "        )\n",
    "        modelfiles: Mapped[List[\"ModelFile\"]] = relationship(\n",
    "            secondary=lambda: analysis_modelfile_table, back_populates=\"analyses\"\n",
    "        )\n",
    "\n",
    "        class HistoLossFile(CommonMixin, Base):\n",
    " \n",
    "        analyses: Mapped[List[Analysis]] = relationship(\n",
    "            secondary=lambda: analysis_histolossfile_table, back_populates=\"histolossfiles\"\n",
    "        )\n",
    "\n",
    "  - \n",
    "\n",
    "      class ModelFile(CommonMixin, Base):\n",
    "            \"\"\"Base class for model files.\"\"\"\n",
    "        \n",
    "            id: Mapped[int] = mapped_column(primary_key=True)\n",
    "            model_type: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "            years_simulated: Mapped[int] = mapped_column(nullable=False)\n",
    "        \n",
    "            client_id: Mapped[int] = mapped_column(ForeignKey(\"client.id\"), nullable=False)\n",
    "            client: Mapped[\"Client\"] = relationship(back_populates=\"modelfiles\")\n",
    "        \n",
    "            yearlosses: Mapped[List[\"ModelYearLoss\"]] = relationship(\n",
    "                back_populates=\"modelfile\",\n",
    "                cascade=\"all, delete-orphan\",\n",
    "            )\n",
    "        \n",
    "            analyses: Mapped[List[Analysis]] = relationship(\n",
    "                secondary=lambda: analysis_modelfile_table, back_populates=\"modelfiles\"\n",
    "            )\n",
    "    \n",
    "\n",
    "- Cascade delete, all for client-analysis:\n",
    "\n",
    "\n",
    "    class Client(CommonMixin, Base):\n",
    "        \"\"\"Represents a client entity.\"\"\"\n",
    "    \n",
    "        id: Mapped[int] = mapped_column(primary_key=True)\n",
    "        name: Mapped[str] = mapped_column(String(50), nullable=False)\n",
    "    \n",
    "        analyses: Mapped[List[\"Analysis\"]] = relationship(\n",
    "            back_populates=\"client\", cascade=\"all, delete-orphan\"\n",
    "        )\n",
    "\n",
    "- The Pydantic classes FrequencyInput and SeverityInput need to be reviewed and refactored\n",
    "- The attribute treshold is missing in the FrequencyModel class\n",
    "- Start frequency and severity models parameters with index 0\n",
    "- The threshold of frequency and severity models is that of the related frequency_severity_model => Remove attribute threshold from severity model\n",
    "- Use ModelType in polymorphic identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd4bb3-3492-4dd5-95f9-bf2380809997",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd7511-27f7-4d3a-bbd8-464a1aa82cea",
   "metadata": {},
   "source": [
    "### Enumerations and Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df406879-7e64-4964-b0c1-52e604df0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ModelType(Enum):\n",
    "    \"\"\"Defines the supported loss models.\"\"\"\n",
    "\n",
    "    EMPIRICAL = \"empirical\"\n",
    "    # FREQUENCY_SEVERITY = \"frequency_severity\"\n",
    "    FREQUENCY_SEVERITY = \"frequencyseveritymodel\"\n",
    "    COMPOSITE_FREQUENCY_SEVERITY = \"composite_frequency_severity\"\n",
    "    EXPOSURE_BASED = \"exposure_based\"\n",
    "\n",
    "\n",
    "class DistributionType(Enum):\n",
    "    \"\"\"Defines the supported statistical distributions.\"\"\"\n",
    "\n",
    "    POISSON = \"poisson\"\n",
    "    NEGATIVE_BINOMIAL = \"negative_binomial\"\n",
    "    PARETO = \"pareto\"\n",
    "\n",
    "\n",
    "class LossType(Enum):\n",
    "    \"\"\"Defines the loss types.\"\"\"\n",
    "\n",
    "    CAT = \"cat\"\n",
    "    NON_CAT = \"non_cat\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DistributionInput:\n",
    "    \"\"\"\n",
    "    Configuration for a statistical distribution.\n",
    "\n",
    "    Attributes:\n",
    "        dist: The distribution type (enum).\n",
    "        params: Parameters specific to the distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    dist: DistributionType\n",
    "    params: list[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b719d63-a33c-486c-9310-942158abbf19",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ecd867-201c-4729-8d42-2bfa52208eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen distribution: pareto\n",
      "Parameters: [2]\n",
      "Sample: [1125.5012521  1012.1873085  2370.44509137 ... 1345.28046044 2672.56236271\n",
      " 2106.53195184]\n",
      "Sample's mean: 2004.4712251830867\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson, nbinom, pareto\n",
    "\n",
    "\n",
    "# Main function to generate model year loss\n",
    "def get_modelyearloss_frequency_severity(\n",
    "    threshold: float,\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    simulated_years: int,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate loss data for a frequency-severity model over a number of simulated years.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Multiplier applied to the severity losses.\n",
    "        frequency_input (DistributionInput): Distribution defining the frequency of events per year.\n",
    "        severity_input (DistributionInput): Distribution defining the severity of each event.\n",
    "        simulated_years (int): Number of years to simulate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"year\": List of years for each loss event.\n",
    "            - \"day\": Random day of the year for each loss.\n",
    "            - \"loss\": Calculated loss values.\n",
    "            - \"loss_type\": Type of loss (e.g., catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    frequencies = generate_frequencies(frequency_input, simulated_years)\n",
    "    years = generate_years(frequencies)\n",
    "    loss_count = len(years)\n",
    "    days = generate_days(loss_count)\n",
    "    losses = threshold * get_sample_from_dist(\n",
    "        severity_input.dist, severity_input.params, loss_count\n",
    "    )\n",
    "    loss_types = generate_loss_types(loss_count)\n",
    "\n",
    "    modelyearloss = {\n",
    "        \"year\": years,\n",
    "        \"day\": days,\n",
    "        \"loss\": losses,\n",
    "        \"loss_type\": loss_types,\n",
    "    }\n",
    "    return modelyearloss\n",
    "\n",
    "\n",
    "def generate_frequencies(\n",
    "    frequency_input: DistributionInput,\n",
    "    size: int,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate a list of event frequencies based on a specified distribution.\n",
    "\n",
    "    Args:\n",
    "        frequency_input (DistributionInput): The distribution and parameters for frequency generation.\n",
    "        size (int): Number of values to generate.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of event frequencies for each simulated year.\n",
    "    \"\"\"\n",
    "    frequencies = get_sample_from_dist(\n",
    "        frequency_input.dist, frequency_input.params, simulated_years\n",
    "    )\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def generate_years(frequencies: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate a list of years for loss events based on event frequencies.\n",
    "\n",
    "    Args:\n",
    "        frequencies (list[int]): A list where each element represents the number of events in a year.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of years, repeated according to their respective frequencies.\n",
    "    \"\"\"\n",
    "    years = [\n",
    "        year for year, freq in enumerate(frequencies, start=1) for _ in range(freq)\n",
    "    ]\n",
    "    return years\n",
    "\n",
    "\n",
    "def generate_days(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random days of the year for loss events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of days to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of random integers representing days (1 to 365).\n",
    "    \"\"\"\n",
    "    days = np.random.randint(1, 366, size)\n",
    "    return days\n",
    "\n",
    "\n",
    "def generate_loss_types(size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random loss types (catastrophic or non-catastrophic) for events.\n",
    "\n",
    "    Args:\n",
    "        size (int): Number of loss types to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of randomly chosen loss types (catastrophic or non-catastrophic).\n",
    "    \"\"\"\n",
    "    loss_types = np.random.choice([LossType.CAT.value, LossType.NON_CAT.value], size)\n",
    "    return loss_types\n",
    "\n",
    "\n",
    "def get_sample_from_dist(\n",
    "    dist: DistributionType, params: list[float], size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a sample from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        dist (DistributionType): The distribution type (e.g., Poisson, Negative Binomial, Pareto).\n",
    "        params (list[float]): Parameters for the selected distribution.\n",
    "        size (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of samples drawn from the specified distribution.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the distribution is not supported.\n",
    "    \"\"\"\n",
    "    match dist:\n",
    "        case DistributionType.POISSON:\n",
    "            return poisson.rvs(mu=params[0], size=size)\n",
    "        case DistributionType.NEGATIVE_BINOMIAL:\n",
    "            return nbinom.rvs(n=params[0], p=params[1], size=size)\n",
    "        case DistributionType.PARETO:\n",
    "            return pareto.rvs(b=params[0], size=size)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported distribution: {dist}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dist = DistributionType.PARETO  # Use Enum for distributions\n",
    "params = [2]\n",
    "sample = 1000 * get_sample_from_dist(dist, params, size=1_000_000)\n",
    "\n",
    "# Output for debugging\n",
    "print(f\"Chosen distribution: {dist.value}\")\n",
    "print(f\"Parameters: {params}\")\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Sample's mean: {sample.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429868bf-6f9f-46d9-bba1-e368c895637b",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9d8b5b-5d6f-475b-a706-08d80e849f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration = 0.718647300000157\n",
      "Average Loss = 1997.7024869338081\n",
      "Frequency = 2.998155\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "threshold = 1000\n",
    "\n",
    "frequency_input = DistributionInput(\n",
    "    dist=DistributionType.POISSON,\n",
    "    params=[3],\n",
    ")\n",
    "\n",
    "severity_input = DistributionInput(\n",
    "    dist=DistributionType.PARETO,\n",
    "    params=[2],\n",
    ")\n",
    "\n",
    "simulated_years = 1_000_000\n",
    "\n",
    "start = time.perf_counter()\n",
    "dict_modelyearloss = get_modelyearloss_frequency_severity(\n",
    "    threshold,\n",
    "    frequency_input,\n",
    "    severity_input,\n",
    "    simulated_years,\n",
    ")\n",
    "print(f\"Duration = {time.perf_counter() - start}\")\n",
    "print(f\"Average Loss = {dict_modelyearloss['loss'].mean()}\")\n",
    "print(f\"Frequency = {len(dict_modelyearloss['loss']) / simulated_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846cfc-7582-4e2d-9e4d-0c700173f432",
   "metadata": {},
   "source": [
    "## Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ede9e2-fef5-42b7-89fb-e71d04e33f4c",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b3a4c5-d333-4bfe-b858-38547d7e2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_from_dict(data: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Converts a dictionary with lists of values into a list of records.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary where keys represent column names and\n",
    "                     values are lists representing rows.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each representing a record.\n",
    "    \"\"\"\n",
    "    records = [dict(zip(data.keys(), values)) for values in zip(*data.values())]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65c16-cc0e-422f-9e1c-814d0e028952",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3ca2d7-9b01-49a0-9eb9-17a2ac38dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sqlalchemy import insert, select, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "from database import (\n",
    "    Analysis,\n",
    "    Client,\n",
    "    HistoLossFile,\n",
    "    ModelFile,\n",
    "    ModelYearLoss,\n",
    "    FrequencyModel,\n",
    "    SeverityModel,\n",
    "    FrequencySeverityModel,\n",
    "    session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98ebb8-59f9-4f83-9997-60d5b5f0c1ba",
   "metadata": {},
   "source": [
    "### Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06779b1-2cfc-440e-a796-68faed7d1994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 'AXA' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a client\n",
    "\n",
    "# Input data\n",
    "client_name = \"AXA\"\n",
    "\n",
    "try:\n",
    "    client = Client(name=client_name)\n",
    "    session.add(client)\n",
    "    session.commit()\n",
    "    print(f\"Client '{client_name}' added successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    session.rollback()\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An unexpected error occured: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdccaa70-36f4-4c0a-8145-43e0e2f6a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create an analysis and associate it with a client\n",
    "\n",
    "# Input data\n",
    "client_id = 1\n",
    "\n",
    "try:\n",
    "    client = session.get_one(Client, client_id)\n",
    "    analysis = Analysis()\n",
    "    client.analyses.append(analysis)\n",
    "    session.commit()\n",
    "    print(f\"Analysis added successfully\")\n",
    "except SQLAlchemyError as e:\n",
    "    session.rollback()\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An unexpected error occured: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6fcaff-1103-438d-a0de-0202fc93e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical loss file added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create a historical loss file and associate it with a client and an analysis\n",
    "\n",
    "# Input data\n",
    "analysis_id = 1\n",
    "\n",
    "try:\n",
    "    # Retrieve the analysis and the associated client\n",
    "    analysis = session.get_one(Analysis, analysis_id)\n",
    "    client = analysis.client\n",
    "\n",
    "    # Create the historical loss file\n",
    "    histolossfile = HistoLossFile()\n",
    "\n",
    "    # Associate the historical loss file with the client and analysis\n",
    "    client.histolossfiles.append(histolossfile)\n",
    "    analysis.histolossfiles.append(histolossfile)\n",
    "\n",
    "    session.commit()\n",
    "    print(f\"Historical loss file added successfully\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    session.rollback()\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An unexpected error occured: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253452c4-1df0-4bf4-8e40-cb8a2daf8c4b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b984f2-74c8-4c0e-9220-34dcb6865016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "def create_frequency_severity_model(\n",
    "    session: Session,\n",
    "    analysis_id: int,\n",
    "    lossfile_id: int,\n",
    "    threshold: float,\n",
    "    frequency_input: DistributionInput,\n",
    "    severity_input: DistributionInput,\n",
    "    years_simulated: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a frequency-severity model and persists related data in the database.\n",
    "\n",
    "    Args:\n",
    "        session (Session): The SQLAlchemy session to use for database operations.\n",
    "        analysis_id (int): ID of the analysis to associate the model with.\n",
    "        lossfile_id (int): ID of the loss file to associate the model with.\n",
    "        threshold (float): Threshold parameter for the model.\n",
    "        frequency_input (FrequencyInput): Input parameters for the frequency model.\n",
    "        severity_input (SeverityInput): Input parameters for the severity model.\n",
    "        years_simulated (int): Number of years simulated for the model.\n",
    "\n",
    "    Raises:\n",
    "        SQLAlchemyError: If a database error occurs during the process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch analysis and client information\n",
    "        analysis = session.get(Analysis, analysis_id)\n",
    "        if not analysis:\n",
    "            raise ValueError(f\"Analysis with ID {analysis_id} not found.\")\n",
    "        client_id = analysis.client_id\n",
    "\n",
    "        # Create frequency and severity models\n",
    "        frequencymodel = FrequencyModel(\n",
    "            **{\n",
    "                f\"parameter_{i}\": param\n",
    "                for i, param in enumerate(frequency_input.params)\n",
    "            }\n",
    "        )\n",
    "        severitymodel = SeverityModel(\n",
    "            **{f\"parameter_{i}\": param for i, param in enumerate(severity_input.params)}\n",
    "        )\n",
    "\n",
    "        # Prepare the frequency-severity model file\n",
    "        modelfile = FrequencySeverityModel(\n",
    "            model_type=\"frequencyseveritymodel\",\n",
    "            threshold=threshold,\n",
    "            years_simulated=years_simulated,\n",
    "            lossfile_id=lossfile_id,\n",
    "            frequencymodel=frequencymodel,\n",
    "            severitymodel=severitymodel,\n",
    "            client_id=client_id,\n",
    "        )\n",
    "\n",
    "        # Link the model file to the analysis\n",
    "        analysis.modelfiles.append(modelfile)\n",
    "        session.flush()  # Flush to generate modelfile ID\n",
    "\n",
    "        # Generate and insert the year loss data\n",
    "        start = time.perf_counter()\n",
    "        modelyearloss = get_modelyearloss_frequency_severity(\n",
    "            threshold, frequency_input, severity_input, years_simulated\n",
    "        )\n",
    "        duration = time.perf_counter() - start\n",
    "        print(f\"Time to get modelyearloss: {duration}\")\n",
    "        start = time.perf_counter()\n",
    "        modelyearloss_records = [\n",
    "            {**loss, \"modelfile_id\": modelfile.id}\n",
    "            for loss in get_records_from_dict(modelyearloss)\n",
    "        ]\n",
    "        duration = time.perf_counter() - start\n",
    "        print(f\"Time to get modelyearloss records: {duration}\")\n",
    "        start = time.perf_counter()\n",
    "        session.execute(insert(ModelYearLoss), modelyearloss_records)\n",
    "        duration = time.perf_counter() - start\n",
    "        print(f\"Time to insert modelyearloss records: {duration}\")\n",
    "\n",
    "        # Commit\n",
    "        start = time.perf_counter()\n",
    "        session.commit()\n",
    "        duration = time.perf_counter() - start\n",
    "        print(f\"Time to commit: {duration}\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()\n",
    "        print(f\"Database error occurred: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75de26d-c2f5-4b3c-909a-55954c478327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get modelyearloss: 0.7039654000000155\n",
      "Time to get modelyearloss records: 5.310707600000114\n",
      "Time to insert modelyearloss records: 29.902278999999908\n",
      "Time to commit: 0.22154219999993074\n",
      "Total Duration = 36.6916025999999\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "start = time.perf_counter()\n",
    "create_frequency_severity_model(\n",
    "    session,\n",
    "    analysis_id=1,\n",
    "    lossfile_id=1,\n",
    "    threshold=1000,\n",
    "    frequency_input=DistributionInput(\n",
    "        dist=DistributionType.POISSON,\n",
    "        params=[3, 0, 0, 0, 0],\n",
    "    ),\n",
    "    severity_input=DistributionInput(\n",
    "        dist=DistributionType.PARETO,\n",
    "        params=[2, 0, 0, 0, 0],\n",
    "    ),\n",
    "    years_simulated=100_000,\n",
    ")\n",
    "\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Total Duration = {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45bc9f3b-aa6e-4009-ae3a-5b39aaee821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The historical loss file has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete a historical loss file\n",
    "\n",
    "# Input data\n",
    "histlossfile_id = 3\n",
    "\n",
    "try:\n",
    "    histolossfile = session.get(HistoLossFile, histlossfile_id)\n",
    "    if not histolossfile:\n",
    "            raise ValueError(f\"Historical loss file with ID {histlossfile_id} not found.\")\n",
    "    session.delete(histolossfile)\n",
    "    session.commit()\n",
    "    print(f\"The historical loss file has been deleted.\")\n",
    "    \n",
    "except SQLAlchemyError as e:\n",
    "    session.rollback()\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An unexpected error occured: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c99b32e-7e13-4f31-a694-760b0e24adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm = session.scalars(\n",
    "    select(FrequencySeverityModel)\n",
    ").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40da31df-8625-40b9-9ffa-0a0e19d9584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrequencySeverityModel(id=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01d037d5-4c79-47eb-9cdd-072a18231ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.delete(fsm)\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An error occured: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    session.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9089e6-e918-47e4-94cf-0560d2325a4e",
   "metadata": {},
   "source": [
    "# DELETE A FREQUENCY SEVERITY MODEL, A FREQUENCY SOLO, A SEVERITY SOLO\n",
    "\n",
    "# TRANSFORM SCRIPTS INTO FUNCTIONS\n",
    "\n",
    "# THEN CORRECT DAYS RECORDING IN THE DATABASE\n",
    "\n",
    "# THEN CREATE JIRA SPECIFIC ISSUES\n",
    "\n",
    "# USE ASYNCIO??? (ASK CHATGPT)\n",
    "\n",
    "# THEN WORKSHOP WITH ANTOINE B TO REVIEW CHANGES LIKE THOSE IN PYDANTIC FOR FREQUENCYINPUT SEVERITYINPUT ETC\n",
    "\n",
    "# THEN DO UI FOR TRAIN_SQLA FOR  ENGINE ONLY FOR ACTUARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d4b84-481c-4ab5-87a4-9eda08108598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
